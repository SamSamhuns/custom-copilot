"""
Openai chat api
"""
import os
from typing import Tuple

import openai
from flask import jsonify
from dotenv import load_dotenv


load_dotenv()
# client = openai.AsyncOpenAI(
#     # This is the default and can be omitted
#     api_key=os.environ.get("OPENAI_API_KEY"),
# )
SEP_TOKEN = "<CHAT_SEP>"  # fixed in media/main.js


async def send_prompt_to_openai(
        prompt: str,
        model: str = "gpt-3.5-turbo",
        max_tokens: int = 150,
        temperature: float = 0.7) -> Tuple[dict, int]:
    """
    Send the request to OpenAI's API
    return repsonse
    """
    # change prompt format to openai chat completion format
    system_msg = {
        "role": "system",
        "content": "You are a helpful assistant that tends to the user's requests closely."
    }
    # sep user & llm msgs by SEP_TOKEN
    messages = [p.strip() for p in prompt.split(SEP_TOKEN)]
    # get seq of user and assistant msgs
    messages = [
        {"role": "user" if msg[:3] == "You" else "assistant",
         "content": msg[5:]} for msg in messages
    ]
    messages = [system_msg] + messages
    print(messages)
    # try:
    #     response = await client.chat.completions.create(
    #         model=model,
    #         messages=[
    #             {
    #                 "role": "user",
    #                 "content": "How do I output all files in a directory using Python?",
    #             },
    #         ],
    #         max_tokens=max_tokens,
    #         temperature=temperature
    #     )

    #     # Extract the text generated by the model
    #     llm_response = response.choices[0].message.content
    #     # llm_response = response.get("choices")[0].get("text").strip()
    # except Exception as e:
    #     print(f"Failed to communicate with OpenAI API: {e}")
    #     return jsonify({"error": "API communication failed"}), 500
    llm_response = {"choices": [{"text": "sample response from llm"}]}
    llm_response = llm_response.get("choices")[0].get("text").strip()
    return jsonify({"response": llm_response}), 200
