"""
Openai chat api
"""
import os
from typing import Tuple

import openai
from flask import jsonify
from dotenv import load_dotenv
from flask_server.chat.utils import num_tokens_from_messages


load_dotenv()
client = openai.AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)
SEP_TOKEN = "<CHAT_SEP>"  # fixed in media/main.js


async def send_prompt_to_openai(
        prompt: str,
        model: str = "gpt-3.5-turbo-0125",
        max_tokens: int = 150,
        temperature: float = 0.7,
        frequency_penalty: float = 1.0) -> Tuple[dict, int]:
    """
    Send the request to OpenAI's API
    return repsonse

    Ref: (https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models)
    frequency_penalty: Penalizes tokens based on their frequency, reducing repetition.
    logit_bias: Modifies likelihood of specified tokens with bias values.
    logprobs: Returns log probabilities of output tokens if true.
    top_logprobs: Specifies the number of most likely tokens to return at each position.
    max_tokens: Sets the maximum number of generated tokens in chat completion.
    n: Generates a specified number of chat completion choices for each input.
    presence_penalty: Penalizes new tokens based on their presence in the text.
    response_format: Specifies the output format, e.g., JSON mode.
    seed: Ensures deterministic sampling with a specified seed.
    stop: Specifies up to 4 sequences where the API should stop generating tokens.
    stream: Sends partial message deltas as tokens become available.
    temperature: Sets the sampling temperature between 0 and 2.
    top_p: Uses nucleus sampling; considers tokens with top_p probability mass.
    tools: Lists functions the model may call.
    tool_choice: Controls the model's function calls (none/auto/function).
    user: Unique identifier for end-user monitoring and abuse detection.
    """
    # change prompt format to openai chat completion format
    system_msg = {
        "role": "system",
        "content": "You are a helpful assistant."
    }
    # sep user & llm msgs by SEP_TOKEN
    messages = [p.strip() for p in prompt.split(SEP_TOKEN)]
    # get seq of user and assistant msgs
    messages = [
        {"role": "user" if msg[:3] == "You" else "assistant",
         "content": msg[5:]} for msg in messages
    ]
    messages = [system_msg] + messages
    ntokens = num_tokens_from_messages(messages, model)
    print(f"Num of tokens send to openai: {ntokens}")

    try:
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            frequency_penalty=frequency_penalty
        )
        # Extract the text generated by the model
        llm_response = response.choices[0].message.content
    except Exception as e:
        print(f"Failed to communicate with OpenAI API: {e}")
        return jsonify({"error": "API communication failed"}), 500

    return jsonify({"response": llm_response}), 200
